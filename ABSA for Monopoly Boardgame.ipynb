{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "178f6b69",
   "metadata": {},
   "source": [
    "# MONOPOLY COMMENT TEXT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31232a",
   "metadata": {},
   "source": [
    "# Packages and dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "#plot packages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for parsing XML\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET   \n",
    "import xmltodict, json\n",
    "import pprint\n",
    "\n",
    "#text mining package\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from langdetect import detect\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#import function used to create the dataframe\n",
    "from ipynb.fs.full.Functions_dataset import extract_film_info\n",
    "from ipynb.fs.full.Functions_dataset import extract_comment_rating\n",
    "from ipynb.fs.full.Functions_dataset import clean_data\n",
    "from ipynb.fs.full.Functions_dataset import return_dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d89493e",
   "metadata": {},
   "source": [
    "#import the dataframe\n",
    "df = pd.DataFrame() \n",
    "\n",
    "for i in range(1,300):\n",
    "    text = str(i)\n",
    "    first = \"https://www.boardgamegeek.com/xmlapi2/thing?id=1406&type=boardgame&ratingcomments=1&page=1\"\n",
    "    second = \"https://www.boardgamegeek.com/xmlapi2/thing?id=1406&type=boardgame&ratingcomments=1&page=\"+text\n",
    "    print(second)\n",
    "    df_add = return_dataset(first, second)\n",
    "    df = df.append(df_add)\n",
    "    \n",
    "df.to_csv(r'C:\\Users\\ASUS\\Desktop\\DSE\\2. Text mining and sentiment analysis\\project\\monopoly_set.csv', index=False) \n",
    "\n",
    "#single attempt\n",
    "#df = return_dataset(\"https://www.boardgamegeek.com/xmlapi2/thing?id=1406&type=boardgame&ratingcomments=1&page=1\", \n",
    "#                    \"https://www.boardgamegeek.com/xmlapi2/thing?id=1406&type=boardgame&ratingcomments=1&page=1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d9229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "df = pd.read_csv(r'C:\\Users\\ASUS\\Desktop\\DSE\\2. Text mining and sentiment analysis\\project\\monopoly_set.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c5073",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7178ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove omoticons from commments\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-english comments \n",
    "words = set(nltk.corpus.words.words())\n",
    "def remove_non_english_comment(comment):\n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(comment) \n",
    "         if w.lower() in words or not w.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_cleaning(column):\n",
    "    column = column.str.replace('[^\\w\\s]','')\n",
    "    column = column.apply(lambda x: remove_emoji(x))\n",
    "    column= column.apply(remove_non_english_comment)\n",
    "    #remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    column = column.apply(lambda x: \" \".join(x.lower() for x in x.split() if x not in stop))\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef90ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-order the dataset needed for the analysis\n",
    "df['value_txt'] = df['value'].astype(str)\n",
    "df['comment'] = comment_cleaning(df['value_txt'])\n",
    "df.dropna(subset=['comment'], inplace=True)\n",
    "df = df.drop('value', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "df['commnent'] = df.comment.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4dbe49",
   "metadata": {},
   "source": [
    "# Some text understanding and data vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The usual wordcloud  to understand what are the most used words in our dataset\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(df['comment'].values))\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=300, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "image = wordcloud.to_image()\n",
    "\n",
    "#wordcloud.to_file('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/word_cloud.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae461f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequencies of words\n",
    "df_hist = df[[\"@username\", \"comment\"]]\n",
    "\n",
    "df_hist.drop_duplicates(subset=[\"@username\"], keep='first', inplace=True)\n",
    "df_hist = df_hist.reset_index()\n",
    "\n",
    "# Split the text into a list of words\n",
    "df_hist['histogram'] = df_hist['comment'].str.split()\n",
    "\n",
    "# Get the frequency of each word\n",
    "word_counts = df_hist['histogram'].apply(lambda x: pd.Series(x).value_counts()).sum()\n",
    "# Get the 20 most frequent words\n",
    "top_20_words = word_counts.nlargest(15)\n",
    "\n",
    "# Plot the histogram\n",
    "top_20_words.plot.bar(figsize=(10,8))\n",
    "\n",
    "# Add labels and show the plot\n",
    "plt.xlabel('Words', fontsize = 16)\n",
    "plt.ylabel('Frequency', fontsize = 16)\n",
    "plt.title('Frequency of 15 Most Common Words', fontsize = 18)\n",
    "plt.xticks(rotation=25, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "#plt.savefig('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/frequent_words.jpeg')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequencies of bigrams\n",
    "df_hist_bi = df[[\"@username\", \"comment\"]]\n",
    "\n",
    "df_hist_bi.drop_duplicates(subset=[\"@username\"], keep='first', inplace=True)\n",
    "df_hist_bi = df_hist_bi.reset_index()\n",
    "\n",
    "# Tokenize the text in the column\n",
    "df_hist_bi['tokens'] = df_hist_bi['comment'].apply(word_tokenize)\n",
    "\n",
    "# Find the bigrams\n",
    "finder = BigramCollocationFinder.from_documents(df_hist_bi['tokens'])\n",
    "\n",
    "# Get the frequency of each bigram\n",
    "bigram_counts = finder.ngram_fd.items()\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "bigram_df = pd.DataFrame(bigram_counts, columns=['bigram', 'count']).sort_values(by='count',ascending=False)\n",
    "top_20_bigrams = bigram_df.nlargest(15, 'count')\n",
    "\n",
    "# Define the function to convert tuple to string\n",
    "def convert_to_string(x):\n",
    "    return ' '.join(x)\n",
    "# Apply the function to the 'words' column\n",
    "top_20_bigrams['sentence'] = top_20_bigrams['bigram'].apply(convert_to_string)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.bar(top_20_bigrams['sentence'], top_20_bigrams['count'], color='green')\n",
    "plt.ylabel('Frequency', fontsize = 14)\n",
    "plt.title('Frequency of 15 Most Common Bigrams', fontsize = 18)\n",
    "plt.xticks(rotation=25, fontsize = 10)\n",
    "\n",
    "#plt.savefig('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/frequent_bigrams.jpeg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigrams\n",
    "df_hist_tri = df[[\"@username\", \"comment\"]]\n",
    "\n",
    "df_hist_tri.drop_duplicates(subset=[\"@username\"], keep='first', inplace=True)\n",
    "df_hist_tri = df_hist_tri.reset_index()\n",
    "\n",
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "\n",
    "# Tokenize the text in the column\n",
    "df_hist_tri['tokens'] = df_hist_tri['comment'].apply(word_tokenize)\n",
    "\n",
    "# Find the trigrams\n",
    "finder = TrigramCollocationFinder.from_documents(df_hist_tri['tokens'])\n",
    "\n",
    "# Get the frequency of each trigram\n",
    "trigram_counts = finder.ngram_fd.items()\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "trigram_df = pd.DataFrame(trigram_counts, columns=['trigram', 'count']).sort_values(by='count',ascending=False)\n",
    "\n",
    "# Plot the histogram\n",
    "top_20_trigrams = trigram_df.nlargest(15, 'count')\n",
    "\n",
    "# Define the function to convert tuple to string\n",
    "def convert_to_string(x):\n",
    "    return ' '.join(x)\n",
    "# Apply the function to the 'words' column\n",
    "top_20_trigrams['sentence'] = top_20_trigrams['trigram'].apply(convert_to_string)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.bar(top_20_trigrams['sentence'], top_20_trigrams['count'],color='red')\n",
    "plt.xlabel('Trigrams',fontsize = 14)\n",
    "plt.ylabel('Frequency',fontsize = 14)\n",
    "plt.title('Frequency of 15 Most Common Trigrams', fontsize = 18)\n",
    "plt.xticks(rotation=25, ha='right', fontsize = 10)\n",
    "plt.yticks(fontsize = 11)\n",
    "\n",
    "#plt.savefig('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/frequent_trigrams.jpeg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating = df[[\"@username\", \"rating\"]]\n",
    "\n",
    "df_rating.drop_duplicates(subset=[\"@username\"], keep='first', inplace=True)\n",
    "df_rating = df_rating.reset_index()\n",
    "\n",
    "#plot distribution of stars\n",
    "\n",
    "df_rating[\"int_rating\"] = df_rating[\"rating\"].astype(int)\n",
    "df_rating[\"rating_txt\"] = df_rating['int_rating'].astype(str)\n",
    "\n",
    "value_counts = df_rating['rating_txt'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "value_counts.plot(kind='bar', color = \"orange\")\n",
    "\n",
    "plt.xlabel('Rating',fontsize = 14)\n",
    "plt.ylabel('Count',fontsize = 14)\n",
    "plt.title('Histogram of Rating frequency', fontsize = 16)\n",
    "plt.xticks(fontsize = 11)\n",
    "plt.yticks(fontsize = 11)\n",
    "\n",
    "#plt.savefig('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/rating_frequency.jpeg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating[\"int_rating\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8826e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043713b",
   "metadata": {},
   "source": [
    "# Aspect Based Santiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121372d",
   "metadata": {},
   "source": [
    "## Simple model: extract most frequent words and compute the mean of ratings of comments in which those words are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.options.display.max_rows = len(df)\n",
    "df = df[[\"@username\", \"rating\", \"boardgame_title\",\"boardgamecategory\", \"boardgamemechanic\", \"comment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract unique comments, delete comments by key\n",
    "df.drop_duplicates(subset=[\"@username\"], keep='first', inplace=True)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list for the association word, adjective\n",
    "lista = []\n",
    "for k in range(len(df)):\n",
    "    for i in nlp(df[\"comment\"][k]):\n",
    "        if i.dep_ == 'nsubj' and (i.pos_ == \"NOUN\" or i.pos_ == \"PROPN\"):\n",
    "            comps = [j for j in i.children if j.pos_ in [\"ADJ\"]]\n",
    "            if comps:\n",
    "                lista.append([comps, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b50eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dataframe structure\n",
    "view = pd.DataFrame(columns=['Adj', 'Noun'])\n",
    "\n",
    "#from element list to datarframe \n",
    "for i in range(len(lista)):\n",
    "    view = view.append({'Adj':lista[i][0], 'Noun': lista[i][1]},ignore_index=True)\n",
    "\n",
    "#type problem for the name, new column\n",
    "view[\"text_name\"] = view['Noun'].astype(str)\n",
    "view = view.drop('Noun', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1007187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the most words used to determine the aspects on which users make comments\n",
    "freq = view['text_name'].value_counts().reset_index(name='count').rename({'index':'word'}, axis = 1)\n",
    "freq = freq.head(15)\n",
    "\n",
    "#list of the most frequent names\n",
    "col_list = freq.word.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127ed4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#restart from the starting df and create a column in which I inserti the words that I found in the respective comment\n",
    "df['found_word'] = df['comment'].apply(lambda x: [i for i in x.split() if i in col_list])\n",
    "\n",
    "df_analisi = df[[\"rating\", \"comment\",\"found_word\"]]\n",
    "\n",
    "df_analisi.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a df in which there is a 1 to 1 relationship with every frequent noun and its relative rating\n",
    "df_analisi = df_analisi.explode(\"found_word\")\n",
    "df_analisi[\"rating_int\"] = df_analisi[\"rating\"].astype(float)\n",
    "df_analisi = df_analisi.reset_index()\n",
    "\n",
    "#create e dictionary in which there is a frequent subject as key and inside the value a list with the rating associated with it\n",
    "result_dict = df_analisi.groupby(\"found_word\")[\"rating_int\"].apply(list).to_dict()\n",
    "\n",
    "# filter the dictionary on the frequent words\n",
    "result_dict = {k:v for k,v in result_dict.items() if k in col_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to understand the distribution of the ratings for each frequent word\n",
    "# Group the data by the category column\n",
    "grouped_data = df_analisi.groupby('found_word')['rating_int'].apply(list)\n",
    "\n",
    "# Create the box plot\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "bp = ax.boxplot(grouped_data.values, labels=grouped_data.index, patch_artist = True)\n",
    "\n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set_color('blue')\n",
    "     \n",
    "for median in bp['medians']:\n",
    "    median.set_color('red')\n",
    "    \n",
    "for patch in bp['boxes']:\n",
    "    patch.set_color('lightblue')\n",
    "    \n",
    "ax.set_ylabel(\"Rating\", fontsize = 14)\n",
    "ax.set_xlabel(\"Subjects\", fontsize = 14)\n",
    "ax.set_title(\"Box Plot with Subject on the X-axis and Rating Distribution on Y-axis\", fontsize = 16)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "#plt.savefig('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/Word_rating_distribution.jpeg')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#not too much information from these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62130a5d",
   "metadata": {},
   "source": [
    "## Sentiment word approach: for each subject in the comments, extract the adjective and return its polarity. Weighted mean of the polarity of each subject for understand the sentiment of a particular word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe the contains the word and mean reating\n",
    "results = []\n",
    "for key, value in result_dict.items():\n",
    "    result = statistics.mean(value)\n",
    "    results.append([key, result])\n",
    "    \n",
    "df_rating = pd.DataFrame(results, columns=['word', 'Mean Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a left join for seing the frequency of a word and the relative rating\n",
    "result = pd.merge(freq, df_rating, on='word', how='left')\n",
    "\n",
    "#ordino il precedente dataframe per rating\n",
    "by_rating = result.sort_values(by='Mean Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca034009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 to 1 correspondence between words and adjectives\n",
    "result1 = pd.merge(by_rating, view, left_on='word', right_on='text_name', how='left')\n",
    "result1 = result1.explode(\"Adj\")\n",
    "\n",
    "#trasformo aggetivo in un oggetto di testo\n",
    "result1[\"text_adj\"] = result1['Adj'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9760c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ordered = result1.sort_values(by='Mean Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6926a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the combo name-adj\n",
    "df_solution = result1.pivot_table(index=['text_name','text_adj'], aggfunc='size').reset_index(name='count')\n",
    "df_grouped = df_solution.groupby('text_name').apply(lambda x: x.sort_values('count', ascending=False))\n",
    "df_grouped_2 = df_grouped.rename(index={'text_name': 'index'})\n",
    "df_analisi_sent = df_grouped_2.sort_values(by='count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "#Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "  \n",
    "#Create two new columns ‘Subjectivity’ & ‘Polarity’\n",
    "df_analisi_sent['polarity'] =  df_analisi_sent['text_adj'].apply(getPolarity)\n",
    "df_analisi_sent['tot_polarity'] = df_analisi_sent['polarity'] * df_analisi_sent['count']\n",
    "\n",
    "df_analisi_sent.reset_index(drop=True, inplace=True)\n",
    "df2 = df_analisi_sent.groupby('text_name')['count', 'tot_polarity'].sum()\n",
    "\n",
    "df2['mean_polarity'] = df2['tot_polarity'] / df2['count']\n",
    "print(df2)\n",
    "\n",
    "\n",
    "df3 = df2.reset_index()\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ab15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_mean = df3[[\"text_name\", \"mean_polarity\"]]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "x = df_plot_mean[\"text_name\"]\n",
    "y = df_plot_mean[\"mean_polarity\"]\n",
    "plt.bar(x, y)\n",
    "\n",
    "plt.xlabel(\"Subjects\", fontsize=14)\n",
    "plt.ylabel(\"Polarity\", fontsize=14)\n",
    "plt.title(\"Bar Plot of Mean Polarity by Subjects\", fontsize = 16)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=25,  fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "#plt.savefig('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/mean_polarity_by_subject.jpeg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37287a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by the category column\n",
    "grouped_data_2 = df_analisi_sent.groupby('text_name')['polarity'].apply(list)\n",
    "\n",
    "# Create the box plot\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "bp = ax.boxplot(grouped_data_2.values, labels=grouped_data_2.index, patch_artist = True)\n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set_color('blue')\n",
    "\n",
    "for median in bp['medians']:\n",
    "    median.set_color('red')\n",
    "    \n",
    "for patch in bp['boxes']:\n",
    "    patch.set_color('lightblue')\n",
    "    \n",
    "ax.set_ylabel(\"Polarity\", fontsize = 14)\n",
    "ax.set_xlabel(\"Subjects\", fontsize = 14)\n",
    "ax.set_title(\"Box Plot with Subjects on X-axis and Polarity on Y-axis\", fontsize = 16)\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=25, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "#plt.savefig('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/polarity_distribution_by_subjects.jpeg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0945d",
   "metadata": {},
   "source": [
    "## Find aspects for the most used words: use pretrained algorithm for vectorize words and make cluster on this words to find possible aspect of the boardgame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained\n",
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088da1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.zeros((15, 25))\n",
    "for i in range(len(df3)):\n",
    "    start[i,:] = glove_vectors[df3['text_name'][i]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ae6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEARCH TOPICS USING UNSUPERVISED LEARNING (K-MEANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c294617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Try different values of k\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(start)\n",
    "    labels = kmeans.predict(start)\n",
    "    score = silhouette_score(start, labels)\n",
    "    print(f'k = {k}, silhouette score = {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc701a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KMeans object with the desired number of clusters\n",
    "import random\n",
    "random.seed(106)\n",
    "kmeans = KMeans(n_clusters=6, random_state = 15)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(start)\n",
    "\n",
    "# Get the cluster assignments for each vector\n",
    "labels = kmeans.predict(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.assign(label=labels)\n",
    "df3[\"text_label\"] = df3['label'].astype(str)\n",
    "df3.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec1322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(start)\n",
    "\n",
    "x = reduced[:, 0]\n",
    "y = reduced[:, 1]\n",
    "\n",
    "df3['x'] = x\n",
    "df3['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d73e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary\n",
    "mapping = {0: 'technical_aspects', 1: 'action_aspects', 2: 'action_aspects', 3: 'general_game_aspects', 4: 'action_aspects', 5: 'monopoly_aspects'}\n",
    "\n",
    "# Add a new column 'B' with the mapped values\n",
    "df3['Aspect'] = df3['label'].map(mapping)\n",
    "df3.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f29ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for key, group in df3.groupby('Aspect'):\n",
    "    plt.scatter(group['x'], group['y'],s=150, label=key)\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"First PCA dimension\", fontsize=12)\n",
    "plt.ylabel(\"Second PCA dimension\", fontsize=12)\n",
    "plt.title(\"Show the association words-aspect\", fontsize = 16)\n",
    "\n",
    "#plt.savefig('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/PCA_words_repr.jpeg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65097de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_group = df3.groupby('Aspect')['mean_polarity'].mean()\n",
    "\n",
    "print(mean_by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analisi_sent2 = pd.merge(df_analisi_sent, df3, on='text_name', how='left')\n",
    "df_analisi_sent_aspect = df_analisi_sent2[[\"tot_polarity_x\",\"count_x\",\"Aspect\", \"polarity\"]]\n",
    "df_analisi_sent2 = df_analisi_sent2[[\"tot_polarity_x\",\"count_x\",\"Aspect\"]]\n",
    "df_analisi_sent2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df_analisi_sent2.groupby('Aspect')['count_x', 'tot_polarity_x'].sum()\n",
    "\n",
    "df4['mean_polarity_aspect'] = df4['tot_polarity_x'] / df4['count_x']\n",
    "print(df4)\n",
    "\n",
    "#inserire un metodo per poter andare a creare una colonna uguale all'indice\n",
    "df5 = df4.reset_index()\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by the category column\n",
    "grouped_data_3 = df_analisi_sent_aspect.groupby('Aspect')['polarity'].apply(list)\n",
    "\n",
    "# Create the box plot\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "bx = ax.boxplot(grouped_data_3.values, labels=grouped_data_3.index, patch_artist = True)\n",
    "\n",
    "for patch in bx['boxes']:\n",
    "    patch.set_color('lightblue')\n",
    "    \n",
    "for whisker in bx['whiskers']:\n",
    "    whisker.set_color('blue')\n",
    "\n",
    "for median in bx['medians']:\n",
    "    median.set_color('red')\n",
    "    \n",
    "ax.set_ylabel(\"Polarity\", fontsize = 14)\n",
    "ax.set_xlabel(\"Aspects\", fontsize = 14)\n",
    "ax.set_title(\"Box Plot with Aspects on X-axis and Polarity on Y-axis\", fontsize = 16)\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=75, fontsize = 10)\n",
    "plt.yticks(fontsize = 10)\n",
    "\n",
    "#plt.savefig('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/polarity distribution_by_aspect.jpeg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_mean2 = df5[[\"Aspect\", \"mean_polarity_aspect\"]]\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "x = df_plot_mean2[\"Aspect\"]\n",
    "y = df_plot_mean2[\"mean_polarity_aspect\"]\n",
    "plt.bar(x, y)\n",
    "\n",
    "plt.xlabel(\"Aspects\", fontsize=14)\n",
    "plt.ylabel(\"Mean Polarity\", fontsize=14)\n",
    "plt.title(\"Bar Plot of Mean Polarity by Aspects\", fontsize = 16)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=10,  fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "\n",
    "#plt.savefig('C:/Users/ASUS/Desktop/DSE/2. Text mining and sentiment analysis/project/plot/mean_polarity_by_aspect.jpeg')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
